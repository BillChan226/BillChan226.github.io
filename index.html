<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<head>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <style type="text/css">
  /* Design Credits: Jon Barron and Deepak Pathak and Abhishek Kar and Saurabh Gupta*/
  a {
  color: #1772d0;
  text-decoration:none;
  }
  a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
  }
  body,td,th {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 400
  }
  heading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 17px; /* 19 */
    font-weight: 600 /* 1000 */
  }
  hr
  {
    border: 0;
    height: 1px;
    background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
  }
  strong {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 16px;
    font-weight: 600 /* 800 */
  }
  strongred {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    color: 'red' ;
    font-size: 16px
  }
  sectionheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    font-weight: 600
  }
  pageheading {
    font-family: 'Titillium Web', Verdana, Helvetica, sans-serif;
    font-size: 38px;
    font-weight: 400
  }
  .ImageBorder
  {
      border-width: 1px;
      border-color: Black;
  }
  </style>
  <link rel="shortcut icon" href="images/aisecure.jpeg">
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Zhaorun Chen</title>
  <meta name="Zhaorun Chen's Homepage" http-equiv="Content-Type" content="Zhaorun Chen's Homepage">
  <link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
  <!-- Start : Google Analytics Code -->
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-XXXXX-Y', 'auto');
    ga('send', 'pageview');
    </script>
  <!-- End : Google Analytics Code -->
  <!-- Scramble Script by Jeff Donahue -->
  <script src="js/scramble.js"></script>
  <meta name="google-site-verification" content="qw6uBrbgeryA92bIE9ljRuQtC72iu7JrsK4jBh3lGbw" />
</head>
 
<body>
<table width="900" border="0" align="center" border="0" cellspacing="0" cellpadding="20">
  <tr><td>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <p align="center">
    <pageheading>Zhaorun Chen</pageheading><br>
  </p>

  <tr>
    <td width="30%" valign="top"><a href="images/personal/IMG_9760.JPG"><img src="images/personal/IMG_9760.JPG" width="95%" style="border-radius:15px"></a>
    <p align=center>
    | <a href="data/CV_zhaorun.pdf">CV</a> |
    <a href="mailto:zhaorun@uchicago.edu">Email</a> |
    <a href="https://scholar.google.com/citations?user=UZg5N5UAAAAJ">Google Scholar</a> |
    <br/>
    | <a href="https://github.com/BillChan226">Github</a> | 
    <a href="https://www.linkedin.com/in/zhaorun-chen-1793b6226/">LinkedIn</a> |
    <!-- <a href="https://space.bilibili.com/14145636">Bilibili</a> |  -->
    </p>
    <p align="center" style="margin-top:-8px;"><iframe id="twitter-widget-0" scrolling="no" frameborder="0" allowtransparency="true" allowfullscreen="true" class="twitter-follow-button twitter-follow-button-rendered" style="position: static; visibility: visible; width: 186px; height: 20px;" title="Twitter Follow Button" src="https://platform.twitter.com/widgets/follow_button.2f70fb173b9000da126c79afe2098f02.en.html#dnt=false&amp;id=twitter-widget-0&amp;lang=en&amp;screen_name=ZRChen_AISafety&amp;show_count=false&amp;show_screen_name=true&amp;size=m&amp;time=1706734206165" data-screen-name=""></iframe><script async="" src="https://platform.twitter.com/widgets.js" charset="utf-8"></script></p>
    </td>
    <td width="70%" valign="top" align="justify">
      <p>I am an incoming Ph.D. student in the <a href="https://aisecure.github.io/">Secure Learning Lab</a> at the <a href="https://cs.uchicago.edu/">Department of Computer Science</a> at <a href="https://www.uchicago.edu/">University of Chicago</a> advised by Prof.<a href="https://aisecure.github.io/"> Bo Li</a>.
      </p>
      <p>Previously, I received my Master degree in <a href="https://engineering.purdue.edu/ECE">Electrical and Computer Engineering</a> at<a href="https://www.purdue.edu/"> Purdue University</a> advised by Prof.<a href="https://engineering.purdue.edu/~lusu/"> Su Lu</a>. Before that, I obtained my Bachelor degree in Automation at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University</a>, advised by Prof.<a href="https://gaoyue.sjtu.edu.cn"> Yue Gao</a>.
         <!-- During 2023 Summer, I interned at <a href="https://cs.unc.edu"> UNC-Chapel Hill</a> advised by Prof.<a href="https://www.huaxiuyao.io/"> Huaxiu Yao</a> and collaborated some wonderful projects with <a href="https://irislab.stanford.edu/people.html"> IRIS Lab</a> hosted by Prof. <a href="https://ai.stanford.edu/~cbfinn/"> Chelsea Finn</a>. -->
      </p>
      <p>My current research interests center on trustworthy deployment and safe interactions with foundation models (e.g. LLMs) and agents from both a theoretical and empirical perspective. Specifically, I’m interested in enhancing their trustworthiness via novel algorithms and certificates for various applications (e.g. <b>hallucination mitigation</b>, <b>human-values alignment</b>, <b>jailbreaks and defense</b>) through incorporating external knowledge sources and LLMs’ reasoning capabilities.
      </p>
      <p><a href="https://billchan226.github.io/publication.html">[Publications]</a> Email: zhaorun [AT] uchicago.edu
      </p>
    </td>
  </tr>
</table>

<hr/>

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
  <td style="padding:20px;width:100%;vertical-align:middle">
    <heading>News</heading>
    <p>
      <ul>
        <!-- <li><b style="color: #2196F3;">[Aug, 2024]</b> 🎉 One first-authored paper accepted by <a href="https://www.corl.org/" style="color: #a200ff;">CoRL 2024</a>!</li>   -->
        <li><b style="color: #2196F3;">[Sept., 2024]</b> 🎉 Two papers accepted by <a href="https://neurips.cc/" style="color: #ff5900;">NeurIPS 2024</a>!</li>
        <li><b style="color: #2196F3;">[July, 2024]</b> 🌟 We are hosting the <a href="https://www.llmagentsafetycomp24.com/" style="color: #f00741;">The LLM and Agent Safety Competition (CLAS 2024)</a> at <a href="https://neurips.cc/" style="color: #f00741;">NeurIPS 2024</a>, stay tuned!</li>
          <!-- <li><b style="color: #2196F3;">[June, 2024]</b> 🏆 One first-authored <a href="https://arxiv.org/pdf/2310.03379" style="color: #ff7300;">paper</a> is accepted by <a href="https://iros2024-abudhabi.org/" style="color: #ff7300;">IROS 2024</a> for <strong>Oral</strong> presentation!</li> -->
          <!-- <li><b style="color: #2196F3;">[June, 2024]</b> 🏆 One paper accepted to <a href="https://2024.acmmm.org/" style="color: #03f59c;">ACM MM 2024</a> for <strong>Oral</strong> presentation!</li> -->
          <li><b style="color: #2196F3;">[May, 2024]</b> 🎉 One first-authored <a href="https://arxiv.org/pdf/2403.00425.pdf" style="color: #2b00ff;">paper</a> accepted by <a href="https://icml.cc/" style="color: #2b00ff;">ICML 2024</a>!</li>
          <li><b style="color: #2196F3;">[Mar., 2024]</b> 🌟 One first-authored <a href="https://arxiv.org/abs/2402.11452" style="color: #9C27B0;">paper</a> accepted by <a href="https://2024.naacl.org/" style="color: #9C27B0;">NAACL 2024</a>!</li>
          <!-- <li><b style="color: #2196F3;">[Feb., 2024]</b> 🎓 Four short-version papers presented at ICLR 2024 Workshops!</li> -->
          <li><b style="color: #2196F3;">[Feb., 2024]</b> 🎉 I've joined <a href="https://aisecure.github.io/" style="color: #FF9800;">Secure Learning Lab</a> at UChicago as a Ph.D. student advised by Prof. Bo Li.</li>
          <!-- <li><b style="color: #2196F3;">[May, 2023]</b> 📜 One paper accepted by <a href="https://www.usenix.org/conference/usenixsecurity23" style="color: #673AB7;">USENIX Security 2023</a>.</li>
          <li><b style="color: #2196F3;">[Feb, 2023]</b> 🏆 I am selected as one of the CS candidates for Conference Presentation Awards for Graduate Students!</li>
          <li><b style="color: #2196F3;">[Nov, 2022]</b> 🌟 One paper accepted by <a href="https://satml.org/" style="color: #9C27B0;">IEEE SatML 2023</a>.</li> -->
          <li><b style="color: #2196F3;">[June, 2023]</b> 🚀 I've joined Prof. Huaxiu Yao at <a href="https://cs.unc.edu" style="color: #CDDC39;">CS Department at UNC-Chapel Hill</a> as a research intern.</li>
          <!-- <li><b style="color: #2196F3;">[Nov., 2021]</b> 📚 First-authored paper received <a href="https://ieeexplore.ieee.org/abstract/document/9597110" style="color: #795548;">Best Paper award</a> at <a href="https://ieeexplore.ieee.org/xpl/conhome/9596614/proceeding" style="color: #795548;">ISRIMT 2021</a>.</li> -->
          <!-- <li><b style="color: #2196F3;">[May, 2021]</b> 🎓 One paper accepted by <a href="https://icml.cc/Conferences/2021" style="color: #9E9E9E;">ICML 2021</a>.</li> -->
      </ul>
    </p>
  </td>
</tr>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Publications</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2407.12784">
      <img src="images/agentpoison/method.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://billchan226.github.io/AgentPoison.html" id="AgentPoison">
      <heading>AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases</heading></a><br>
      <b>Zhaorun Chen</b>, Zhen Xiang, Chaowei Xiao, Dawn Song, and Bo Li<br>
      Advances in Neural Information Processing Systems (NeurIPS), 2024<br>
      </p>

      <div class="paper" id="agentpoison">
      <a href="https://arxiv.org/pdf/2407.12784">pdf</a> |
      <a href="javascript:toggleblock('agentpoison_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('agentpoison')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2310.03379">arXiv</a> 

      <p align="justify"> <i id="agentpoison_abs">LLM agents have demonstrated remarkable performance across various applications, primarily due to their advanced capabilities in reasoning, utilizing external knowledge and tools, calling APIs, and executing actions to interact with environments. Current agents typically utilize a memory module or a retrieval-augmented generation (RAG) mechanism, retrieving past knowledge and instances with similar embeddings from knowledge bases to inform task planning and execution. However, the reliance on unverified knowledge bases raises significant concerns about their safety and trustworthiness. To uncover such vulnerabilities, we propose a novel red teaming approach AgentPoison, the first backdoor attack targeting generic and RAG-based LLM agents by poisoning their long-term memory or RAG knowledge base. In particular, we form the trigger generation process as a constrained optimization to optimize backdoor triggers by mapping the triggered instances to a unique embedding space, so as to ensure that whenever a user instruction contains the optimized backdoor trigger, the malicious demonstrations are retrieved from the poisoned memory or knowledge base with high probability. In the meantime, benign instructions without the trigger will still maintain normal performance. Unlike conventional backdoor attacks, AgentPoison requires no additional model training or fine-tuning, and the optimized backdoor trigger exhibits superior transferability, in-context coherence, and stealthiness. Extensive experiments demonstrate AgentPoison effectiveness in attacking three types of real-world LLM agents: RAG-based autonomous driving agent, knowledge-intensive QA agent, and healthcare EHRAgent. We inject the poisoning instances into the RAG knowledge base and long-term memories of these agents, respectively, demonstrating the generalization of AgentPoison. On each agent, AgentPoison achieves an average attack success rate of ≥ 80% with minimal impact on benign performance (≤ 1%) with a poison rate < 0.1%. Code is released <a href="https://github.com/BillChan226/AgentPoison" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
  @article{chen2024agentpoison,
    title={AgentPoison: Red-teaming LLM Agents via Poisoning Memory or Knowledge Bases},
    author={Chen, Zhaorun and Xiang, Zhen and Xiao, Chaowei and Song, Dawn and Li, Bo},
    journal={arXiv preprint arXiv:2407.12784},
    year={2024}
  }
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/pdf/2405.14622">
      <img src="images/paper/csr.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://dongjie-cheng.github.io/CSR.html" id="CSR">
      <heading>Calibrated Self-Rewarding Vision Language Models</heading></a><br>
      Yiyang Zhou<sup>*</sup>, Zhiyuan Fan<sup>*</sup>, Dongjie Cheng<sup>*</sup>, Sihan Yang, <b>Zhaorun Chen</b>, Chenhang Cui, Xiyao Wang, Yun Li, Linjun Zhang, and Huaxiu Yao<br>
      Advances in Neural Information Processing Systems (NeurIPS), 2024<br>
      </p>

      <div class="paper" id="agentpoison">
      <a href="https://arxiv.org/pdf/2405.14622">pdf</a> |
      <a href="javascript:toggleblock('csr_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('csr')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/pdf/2405.14622">arXiv</a> 

      <p align="justify"> <i id="csr_abs">Large Vision-Language Models (LVLMs) have made substantial progress by integrating pre-trained large language models (LLMs) and vision models through instruction tuning. Despite these advancements, LVLMs often exhibit the hallucination phenomenon, where generated text responses appear linguistically plausible but contradict the input image, indicating a misalignment between image and text pairs. This misalignment arises because the model tends to prioritize textual information over visual input, even when both the language model and visual representations are of high quality. Existing methods leverage additional models or human annotations to curate preference data and enhance modality alignment through preference optimization. These approaches are resource-intensive and may not effectively reflect the target LVLM’s preferences, making the curated preferences easily distinguishable. Our work addresses these challenges by proposing the Calibrated Self-Rewarding (CSR) approach, which enables the model to self-improve by iteratively generating candidate responses, evaluating the reward for each response, and curating preference data for fine-tuning. In the reward modeling, we employ a step-wise strategy and incorporate visual constraints into the self-rewarding process to place greater emphasis on visual input. Empirical results demonstrate that CSR significantly enhances performance and reduces hallucinations across ten benchmarks and tasks, achieving substantial improvements over existing methods by 7.62%. Our empirical results are further supported by rigorous theoretical analysis, under mild assumptions, verifying the effectiveness of introducing visual constraints into the self-rewarding paradigm. Additionally, CSR shows compatibility with different vision-language models and the ability to incrementally improve performance through iterative fine-tuning.</i></p>

<pre xml:space="preserve">
  @article{zhou2024calibrated,
    title={Calibrated self-rewarding vision language models},
    author={Zhou, Yiyang and Fan, Zhiyuan and Cheng, Dongjie and Yang, Sihan and Chen, Zhaorun and Cui, Chenhang and Wang, Xiyao and Li, Yun and Zhang, Linjun and Yao, Huaxiu},
    journal={arXiv preprint arXiv:2405.14622},
    year={2024}
  }
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2403.00425">
      <img src="images/halc/halc.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://billchan226.github.io/HALC.html" id="HALC">
      <heading>HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding</heading></a><br>
      <b>Zhaorun Chen<sup>*</sup></b>, Zhuokai Zhao<sup>*</sup>, Hongyin Luo, Huaxiu Yao, Bo Li and Jiawei Zhou<br>
      International Conference on Machine Learning (ICML), 2024<br>
      <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
      </p>

      <div class="paper" id="halc">
      <!-- <a href="https://manipulation-locomotion.github.io">webpage</a> | -->
      <a href="https://arxiv.org/pdf/2310.03379.pdf">pdf</a> |
      <a href="javascript:toggleblock('halc_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('halc')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2310.03379">arXiv</a> 

      <p align="justify"> <i id="halc_abs">While large vision-language models (LVLMs) have demonstrated impressive capabilities in interpreting multi-modal contexts, they invariably suffer from object hallucinations (OH). We introduce HALC, a novel decoding algorithm designed to mitigate OH in LVLMs. HALC leverages distinct fine-grained optimal visual information in vision-language tasks and operates on both local and global contexts simultaneously. Specifically, HALC integrates a robust auto-focal grounding mechanism (locally) to correct hallucinated tokens on the fly, and a specialized beam search algorithm (globally) to significantly reduce OH while preserving text generation quality. Additionally, HALC can be integrated into any LVLMs as a plug-and-play module without extra training. Extensive experimental studies demonstrate HALC’s effectiveness in reducing OH, outperforming state-of-the-arts across four benchmarks. Code is released <a href="https://github.com/BillChan226/HALC">here</a>.</i></p>

<pre xml:space="preserve">
  @article{chen2024halc,
    title={HALC: Object Hallucination Reduction via Adaptive Focal-Contrast Decoding},
    author={Chen, Zhaorun and Zhao, Zhuokai and Luo, Hongyin and Yao, Huaxiu and Li, Bo and Zhou, Jiawei},
    journal={arXiv preprint arXiv:2403.00425},
    year={2024}
  }
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2407.04842">
      <img src="images/mjbench/overview.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
    </a></td>
    <td width="60%" valign="top">
      <p><a href="https://mj-bench.github.io/" id="mjbench">
      <heading>MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?</heading></a><br>
      <b>Zhaorun Chen<sup>*</sup></b>, Yichao Du<sup>*</sup>, Zichen Wen<sup>*</sup>, Yiyang Zhou<sup>*</sup>, Chenhang Cui, Zhenzhen Weng, Haoqin Tu, Chaoqi Wang, Zhengwei Tong, Qinglan Huang, Canyu Chen, Qinghao Ye, Zhihong Zhu, Yuqing Zhang, Jiawei Zhou, Zhuokai Zhao, Rafael Rafailov, Chelsea Finn, and Huaxiu Yao<br>
      In submission, 2024<br>
      </p>

      <div class="paper" id="mjbench">
      <a href="https://arxiv.org/pdf/2407.04842">pdf</a> |
      <a href="javascript:toggleblock('mjbench_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('mjbench')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2407.04842">arXiv</a> 

      <p align="justify"> <i id="mjbench_abs">Multimodal reward models (RMs) are critical in RLHF and RLAIF, where they serve as judges and provide feedback for aligning foundation models (FMs) with desired behaviors. Despite their significance, these multimodal judges often un- dergo inadequate evaluation of their capabilities and biases, which may lead to potential misalignment and unsafe fine-tuning outcomes. To address this issue, we introduce MJ-Bench, a novel benchmark which incorporates a comprehensive preference dataset to evaluate multimodal judges in providing feedback for image generation models across four key perspectives: alignment, safety, image quality, and bias. Specifically, we evaluate a large variety of multimodal judges includ- ing smaller-sized CLIP-based scoring models, open-source VLMs (e.g. LLaVA family), and close-source VLMs (e.g. GPT-4o, Claude 3) on each decomposed subcategory of our preference dataset. Experiments reveal that close-source VLMs generally provide better feedback, with GPT-4o outperforming other judges in aver- age. Compared with open-source VLMs, smaller-sized scoring models can provide better feedback regarding text-image alignment and image quality, while VLMs provide more accurate feedback regarding safety and generation bias due to their stronger reasoning capabilities. Notably, human evaluations on end-to-end fine- tuned models using separate feedback from these multimodal judges provide similar conclusions, further confirming the effectiveness of MJ-Bench. Further studies in feedback scale reveal that VLM judges can generally provide more accurate and stable feedback in natural language (Likert-scale) than numerical scales. The code and data are available <a href="https://github.com/BillChan226/MJ-Bench" style="color:#36AE7C;">here</a>.</i></p>

<pre xml:space="preserve">
  @misc{chen2024mjbenchmultimodalrewardmodel,
    title={MJ-Bench: Is Your Multimodal Reward Model Really a Good Judge for Text-to-Image Generation?}, 
    author={Zhaorun Chen and Yichao Du and Zichen Wen and Yiyang Zhou and Chenhang Cui and Zhenzhen Weng and Haoqin Tu and Chaoqi Wang and Zhengwei Tong and Qinglan Huang and Canyu Chen and Qinghao Ye and Zhihong Zhu and Yuqing Zhang and Jiawei Zhou and Zhuokai Zhao and Rafael Rafailov and Chelsea Finn and Huaxiu Yao},
    year={2024},
    eprint={2407.04842},
    archivePrefix={arXiv},
    primaryClass={cs.CV},
    url={https://arxiv.org/abs/2407.04842}, 
}
</pre>
      </div>
    </td>
  </tr>

  <tr>
    <td width="40%" valign="top" align="center">
      <a href="https://arxiv.org/abs/2402.11452">
          <img src="images/autoprm/autoprm.png" alt="sym" width="90%" style="padding-top:0px; padding-bottom:0px; border-radius:15px; height: auto;">
      </a>
    </td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2402.11452" id="AUTOPRM">
      <heading>AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition</heading></a><br>
      <b>Zhaorun Chen</b>, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj and Huaxiu Yao<br>
      North American Chapter of the Association for Computational Linguistics (NAACL), 2024<br>
      <!-- short version presented at ICLR 2024 <a href="https://iclr-r2fm.github.io/">R2-FM</a> Workshop<br> -->
      </p>

      <div class="paper" id="autoprm">
      <a href="https://arxiv.org/pdf/2402.11452.pdf">pdf</a> |
      <a href="javascript:toggleblock('autoprm_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('autoprm')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2402.11452">arXiv</a> 

      <p align="justify"> <i id="autoprm_abs">Recent advancements in large language models (LLMs) have shown promise in multi-step reasoning tasks, yet their reliance on extensive manual labeling to provide procedural feedback remains a significant impediment. To address this challenge, in this paper, we propose a novel self-supervised framework AutoPRM that efficiently enhances the fine-tuning of LLMs for intricate reasoning challenges. Specifically, AutoPRM first decomposes complex problems into more manageable subquestions with a controllable granularity switch, then sequentially apply reinforcement learning to iteratively improve the subquestion solver. Additionally, we propose context-guided-decoding to avoid reward tampering and guide the subquestion solver towards the solution of the holistic problem. Extensive experiments show that AutoPRM significantly improves performance on mathematical and commonsense reasoning tasks over SOTA. More encouragingly, AutoPRM can be easily integrated with other orthogonal reasoning pipelines.</i></p>

<pre xml:space="preserve">
  @article{chen2024autoprm,
    title={AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition},
    author={Chen, Zhaorun and Zhao, Zhuokai and Zhu, Zhihong and Zhang, Ruiqi and Li, Xiang and Raj, Bhiksha and Yao, Huaxiu},
    journal={arXiv preprint arXiv:2402.11452},
    year={2024}
  }
</pre>
      </div>
    </td>
  </tr>


  <tr>
    <td width="40%" valign="top" align="center"><a href="https://arxiv.org/abs/2310.03379">
      <video playsinline autoplay loop muted src="images/acs/ACS-Video.mp4" poster="./images/loading-icon.gif" alt="sym" width="90%" style="padding-top:0px;padding-bottom:0px;border-radius:15px;"></video>
      </a></td>
    <td width="60%" valign="top">
      <p><a href="https://arxiv.org/abs/2310.03379" id="ACS">
      <heading>Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards</heading></a><br>
      <b>Zhaorun Chen</b>, Zhuokai Zhao, Tairan He, Binhao Chen, Xuhao Zhao, Liang Gong, Chengliang Liu<br>
      IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2024<br>
      </p>

      <div class="paper" id="acs">
      <!-- <a href="https://manipulation-locomotion.github.io">webpage</a> | -->
      <a href="https://arxiv.org/pdf/2310.03379.pdf">pdf</a> |
      <a href="javascript:toggleblock('acs_abs')">abstract</a> |
      <a shape="rect" href="javascript:togglebib('acs')" class="togglebib">bibtex</a> |
      <a href="https://arxiv.org/abs/2310.03379">arXiv</a> 

      <p align="justify"> <i id="acs_abs">Ensuring safety in Reinforcement Learning (RL), typically framed as a Constrained Markov Decision Process (CMDP), is crucial for real-world exploration applications. Current approaches in handling CMDP struggle to balance optimality and feasibility, as direct optimization methods cannot ensure state-wise in-training safety, and projection-based methods correct actions inefficiently through lengthy iterations. To address these challenges, we propose Adaptive Chance-constrained Safeguards (ACS), an adaptive, model-free safe RL algorithm using the safety recovery rate as a surrogate chance constraint to iteratively ensure safety during exploration and after achieving convergence. Theoretical analysis indicates that the relaxed probabilistic constraint sufficiently guarantees forward invariance to the safe set. And extensive experiments conducted on both simulated and real-world safety-critical tasks demonstrate its effectiveness in enforcing safety (nearly zero-violation) while preserving optimality (+23.8%), robustness, and fast response in stochastic real-world settings.</i></p>

<pre xml:space="preserve">
  @misc{chen2024safereinforcementlearninghierarchical,
    title={Safe Reinforcement Learning via Hierarchical Adaptive Chance-Constraint Safeguards}, 
    author={Zhaorun Chen and Zhuokai Zhao and Tairan He and Binhao Chen and Xuhao Zhao and Liang Gong and Chengliang Liu},
    year={2024},
    eprint={2310.03379},
    archivePrefix={arXiv},
    primaryClass={cs.RO},
    url={https://arxiv.org/abs/2310.03379}, 
}
</pre>
      </div>
    </td>
  </tr>






<table width="100%" align="center" border="0" cellspacing="0" cellpadding="10">
  <tr><td><sectionheading>&nbsp;&nbsp;Reviewer Service</sectionheading></td></tr>
</table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="15">
  <tr>
    <td style="padding:20px;width:100%;vertical-align:middle">
      Conference Reviewer: NeurIPS'24, ICLR'24, COLM'24, ARR'24, IROS'24
      <br>
      Journal Reviewer: Plant Phenomics
      </p>
    </td>
  </tr>
</table>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
  <tbody>
      <tr>
          <td style="padding:0px">
              <br>
              <br>
              <div>
                  <script type="text/javascript" id="clustrmaps" src="//cdn.clustrmaps.com/map_v2.js?cl=080808&w=200&t=tt&d=1LQ8joet7HZOF5z74B4awh9KBpyFKrjXiK4F3y83o_g&co=ffffff&cmo=3acc3a&cmn=ff5353&ct=808080"></script>
                  <!-- <a target="_top" href="http://clustrmaps.com/site/1acpn?utm_source=widget&amp;utm_campaign=widget_ctr" id="clustrmaps-widget-v2" class="clustrmaps-map-control" style="width: 300px;">
-->                               </div>
          </td>
      </tr>
  </tbody>
</table>








<hr/>

<table width="100%" align="center" border="0" cellspacing="0" cellpadding="2">
    <tr><td><br><p align="right">
    Website template from <a href="http://www.cs.berkeley.edu/~barron/">here</a> and <a href="http://www.cs.cmu.edu/~dpathak/">here</a>
    </font></p></td></tr>
</table>

  </td></tr>
</table>
<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('material_review_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('ieee_iot_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('agentpoison_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('csr_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('mjbench_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('halc_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('autoprm_abs');
</script>
<script xml:space="preserve" language="JavaScript">
  hideblock('acs_abs');
</script>
</body>

</html>
