<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Tairan He 何泰然</title>
  
  <meta name="author" content="Tairan He">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
<!--   <link rel="icon" type="image/png" href="images/seal_icon.png"> -->
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Tairan He 「何泰然」</name>
              </p>
              <p>I am a junior undergraduate student at <a href="http://en.sjtu.edu.cn"> Shanghai Jiao Tong University (SJTU)</a> majoring in Computer Science & Technology. 
              </p>
              <p>I have been working as a research intern at <a href="http://apex.sjtu.edu.cn"> APEX Lab</a> since July 2019, advised by <a href="http://wnzhang.net"> Prof. Weinan Zhang</a>.
              </p>
<!--               <p>
                At Google I've worked on <a href="https://ai.googleblog.com/2014/04/lens-blur-in-new-google-camera-app.html">Lens Blur</a>, <a href="https://ai.googleblog.com/2014/10/hdr-low-light-and-high-dynamic-range.html">HDR+</a>, <a href="https://www.google.com/get/cardboard/jump/">Jump</a>, <a href="https://ai.googleblog.com/2017/10/portrait-mode-on-pixel-2-and-pixel-2-xl.html">Portrait Mode</a>, and <a href="https://www.youtube.com/watch?v=JSnB06um5r4">Glass</a>. I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and funded by the <a href="http://www.nsfgrfp.org/">NSF GRFP</a>. I did my bachelors at the <a href="http://cs.toronto.edu">University of Toronto</a>.
                I've received the <a href="https://www2.eecs.berkeley.edu/Students/Awards/15/">C.V. Ramamoorthy Distinguished Research Award</a> and the <a href="https://www.thecvf.com/?page_id=413#YRA">PAMI Young Researcher Award</a>.
              </p> -->
              <br>
              <p style="text-align:center">
                <a href="mailto:tairanhe1999@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/JonBarron-CV.pdf">CV</a> &nbsp/&nbsp
                <a href="https://github.com/TairanHe">Github</a> <!-- &nbsp/&nbsp -->
<!--                 <a href="data/JonBarron-bio.txt">Biography</a> &nbsp/&nbsp -->
<!--                 <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Google Scholar</a>  --><!-- &nbsp/&nbsp -->
<!--                 <a href="https://twitter.com/jon_barron">Twitter</a> -->
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/TairanHe.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/TairanHe_circle.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research Interests</heading>
              <p>
                I'm interested in <b>reinforcement learning</b> and <b>machine learning</b>. Currently, I concetrate on the data-driven reinforcement learning tasks (e.g., <b>imitation learning</b> and <b>batch RL</b>). I keep reading an inspiring paper every day, and share my understanding and notes in the  <a href="https://www.zhihu.com/column/c_1300374076980932608"> Zhihu column</a>.<!-- Representative papers are <span class="highlight">highlighted</span>. -->
              </p>
            </td>
          </tr>
        </tbody></table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Publications</heading>
            </td>
          </tr>
          <tr>
            <td style="padding:0px 20px 20px 20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.09395" id="EBIL">
                <papertitle>Energy-Based Imitation Learning</papertitle>
              </a>
              <p>Minghuan Liu, <b>Tairan He</b>, Minkai Xu, Weinan Zhang </p>
              <em>ArXiv Preprint</em>
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <p>We proposed EBIL, a two-step solution for imitation learning: first estimate the energy of expert’s occupancy measure, and then take the energy to construct a surrogate reward function as a guidance for the agent to learn the desired policy.</p>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>


        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.09395" id="EBIL">
                <papertitle>Energy-Based Imitation Learning</papertitle>
              </a>
              <p>Minghuan Liu, <b>Tairan He</b>, Minkai Xu, Weinan Zhang </p>
              <em>ArXiv Preprint</em>
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <p>We proposed EBIL, a two-step solution for imitation learning: first estimate the energy of expert’s occupancy measure, and then take the energy to construct a surrogate reward function as a guidance for the agent to learn the desired policy.</p>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.09395" id="EBIL">
                <papertitle>Energy-Based Imitation Learning</papertitle>
              </a>
              <p>Minghuan Liu, <b>Tairan He</b>, Minkai Xu, Weinan Zhang </p>
              <em>ArXiv Preprint</em>
<!--               <em>TPAMI</em>, 2017 -->
<!--               <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
              <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
              <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a> -->
<!--               <p></p> -->
              <p>We proposed EBIL, a two-step solution for imitation learning: first estimate the energy of expert’s occupancy measure, and then take the energy to construct a surrogate reward function as a guidance for the agent to learn the desired policy.</p>
<!--               <p>This paper subsumes our CVPR 2014 paper.</p> -->
            </td>
          </tr>
        </tbody></table>



      </td>
    </tr>
  </table>
</body>

</html>
